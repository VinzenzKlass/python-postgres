{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"Python Postgres  <p>Python Postgres aims to simplify PostgreSQL interaction in Python. It's a thin abstraction over psycopg that uses SQL directly, avoiding ORM-specific syntax while offering ORM-like benefits like Pydantic model parsing for expanded editor support, and reduced boilerplate. It retains full SQL flexibility and access to underlying psycopg features.</p> <p>Head over to the Quick Start to get started with the basics.</p> <p>If you find any issues or feel that this SDK is not adequately covering your use case, please open an issue.</p>"},{"location":"installation/","title":"Installation","text":"<p>This Project requires <code>Python&gt;=3.12</code>.</p>"},{"location":"installation/#install","title":"Install","text":"<p>By default, python-postgres will install the pure python version of psycopg and the pool extra. Just like psycopg itself, you can install it with the <code>binary</code> or <code>c</code> extras. Whenever possible, you should prefer the <code>c</code> version, especially for production use cases. Read more about the distribution options in the psycopg documentation.</p> pipuvPoetry <pre><code>pip install python-postgres[c]</code></pre> <pre><code>uv add python-postgres[c]</code></pre> <pre><code>poetry add python-postgres[c]</code></pre> <p>If this fails, you can try the <code>binary</code> or pure-python version.</p> pipuvPoetry <pre><code>pip install python-postgres[binary]</code></pre> <pre><code>pip install python-postgres</code></pre> <pre><code>uv add python-postgres[binary]</code></pre> <pre><code>uv add python-postgres</code></pre> <pre><code>poetry add python-postgres[binary]</code></pre> <pre><code>poetry add python-postgres</code></pre>"},{"location":"quickstart/","title":"Quick Start","text":"Setup <p>To run the Quickstart examples, you will need the following Table:</p> <pre><code>CREATE TABLE comments\n(\n    id         SMALLSERIAL PRIMARY KEY,\n    content    TEXT NOT NULL,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);</code></pre> <p>some values to get started:</p> <pre><code>INSERT INTO comments (content)\nVALUES ('This is a comment'),\n       ('This is another comment');</code></pre> <p>and the following Python setup code:</p> <pre><code>from datetime import datetime\nfrom typing import Optional\n\nfrom pydantic import BaseModel, Field\n\nclass Comment(BaseModel):\n    content: str\n    created_at: Optional[datetime] = Field(default=None)\n    updated_at: Optional[datetime] = Field(default=None)</code></pre>"},{"location":"quickstart/#basic-usage","title":"Basic Usage","text":"<p>You can use the <code>Postgres</code> instance to run queries the same way you would with a <code>psycopg</code> cursor, and it will return results exactly the same way. However, you do not need to create a connection or a cursor yourself. Instantiating <code>Postgres</code> creates a Connection Pool and calling it will acquire a Connection from the Pool, spawn a Cursor on it, and execute your query. After the query, the connection is returned to the Pool. Some examples:</p> <pre><code>from python_postgres import Postgres\n\n\n# TODO: Set your actual credentials\npg = Postgres(\"postgres\", \"password\", \"localhost\")\nawait pg(\n    \"INSERT INTO comments (content) VALUES (%s);\", (\"Hello from Python!\",)\n)  # You can pass a list of tuples to insert multiple rows at once\nraw = await pg(\"SELECT * FROM comments;\")\nprint(raw)</code></pre> See Output <pre><code>[\n    (\n        1,\n        \"This is a comment\",\n        datetime.datetime(2025, 4, 24, 11, 53, 26, 419007),\n        datetime.datetime(2025, 4, 24, 11, 53, 26, 419007),\n    ),\n    (\n        2,\n        \"This is another comment\",\n        datetime.datetime(2025, 4, 24, 11, 53, 26, 419007),\n        datetime.datetime(2025, 4, 24, 11, 53, 26, 419007),\n    ),\n    (\n        3,\n        \"Hello from Python!\",\n        datetime.datetime(2025, 4, 24, 12, 56, 2, 939546),\n        datetime.datetime(2025, 4, 24, 12, 56, 2, 939546),\n    ),\n]</code></pre> Equivalent in <code>psycopg</code> <p>The above code condenses the following <code>psycopg</code> code for you (omitting error handling and handling of other cases):</p> <pre><code>con_str = f\"postgresql://{user}:{quote_plus(password)}@{host}:{port}/{database}\"\ncon_pool = AsyncConnectionPool(con_str, min_size=10, max_size=50, open=False)\n\nawait con_pool.open()\nasync with con_pool.connection() as conn:\n    async with conn.cursor(binary=True) as cur:\n        await cur.execute(\"INSERT INTO comments (content) VALUES (%s);\", (\"Hello from Python!\",))\n        await conn.commit()\n        num_inserted = cur.rowcount\n\n    async with conn.cursor(binary=True) as cur:\n        await cur.execute(\"SELECT * FROM comments;\")\n        raw = await cur.fetchall()</code></pre>"},{"location":"quickstart/#pydantic-models","title":"Pydantic Models","text":"<p>Alternatively, you can pass Pydantic models as query parameters, and you can get results as models. Nested Models or  models with dictionaries as fields are supported as well. If you pass models with either, those fields will get  serialized to the Postgres <code>JSONB</code> type. <code>JSON</code> and <code>JSONB</code> fields are automatically parsed when reading them from the  database. Serializing to <code>JSON</code> is currently not supported. Fields of type <code>list[dict]</code> will be serialized to <code>JSONB</code>  as well.</p> <pre><code>await pg(\n    \"INSERT INTO comments (content, created_at, updated_at) VALUES (%s,%s,%s)\",\n    Comment(\n        content=\"I was a pydantic model.\",\n        created_at=datetime.now(),\n        updated_at=datetime.now(),\n    ),\n)\n# The inferred type of comments will be List[Comment]\ncomments = await pg(\"SELECT * FROM comments;\", model=Comment)\nprint(res)</code></pre> See Output <p>Note that this does not impact your query. The <code>SELECT *</code> query will still return all columns and send them over network, they will just not be included in the result set. <pre><code>[\n    Comment(\n        content=\"This is a comment\",\n        created_at=datetime.datetime(2025, 4, 24, 11, 53, 26, 419007),\n        updated_at=datetime.datetime(2025, 4, 24, 11, 53, 26, 419007),\n    ),\n    ...,\n    Comment(\n        content=\"I was a pydantic model.\",\n        created_at=datetime.datetime(2025, 4, 24, 16, 15, 51, 992358),\n        updated_at=datetime.datetime(2025, 4, 24, 16, 15, 51, 992374),\n    ),\n]</code></pre></p>"},{"location":"quickstart/#dynamic-pydantic-inserts","title":"Dynamic Pydantic Inserts","text":"<p>You can pass a list of Pydantic models to the above query to insert multiple rows at once:</p> <pre><code>comments = [\n    Comment(\n        content=\"This has both created_at and updated_at info.\",\n        created_at=datetime.now(),\n        updated_at=datetime.now(),\n    ),\n    Comment(content=\"This has created_at info.\", created_at=datetime.now()),\n    Comment(content=\"This has only content.\"),\n]\nawait pg(\n    \"INSERT INTO comments (content, created_at, updated_at) VALUES (%s,%s,%s);\",\n    comments,\n)</code></pre> <p>This will run just fine, but has a caveat: the <code>created_at</code> and <code>updated_at</code> fields are nullable in the database and optional in the model. This leads to None values explicitly being passed to the database, derailing the default values and producing these rows:</p> id content created_at updated_at 5 This has both created_at and updated_at info. 2025-04-24 16:43:23.383199 2025-04-24 16:43:23.383217 6 This has created_at info. 2025-04-24 16:43:23.383244 null 7 This has only content. null null <p>This is most likely not the intended behaviour. To address this, you can use the <code>insert()</code> method, which correctly  expands the insert columns based on all non-<code>None</code> fields for each model and avoids passing None values to the database. This still only produces one single query, inserting <code>DEFAULT</code> values where appropriate.</p> <pre><code>await pg.insert(\"comments\", comments)</code></pre> <p>You may not need this</p> <p>Note that this specifically addresses the insertion of non-uniform models. If you are inserting a list of models that all have the same fields, you can still use the regular <code>INSERT INTO</code> syntax and pass the models as parameters directly, which is more efficient and allows for more elaborate Insert queries.</p> <p>This will produce the following rows:</p> id content created_at updated_at 8 This has both created_at and updated_at info. 2025-04-24 17:06:32.316644 2025-04-24 17:06:32.316663 9 This has created_at info. 2025-04-24 17:06:32.316690 2025-04-24 15:06:31.499539 10 This has only content. 2025-04-24 15:06:31.499539 2025-04-24 15:06:31.499539 <p>With this, the fields that were <code>null</code> before correctly get populated with the default values from the database, as is  evident by the fact that all 3 of them hold identical values.</p>"},{"location":"quickstart/#transactions","title":"Transactions","text":"<p>You can use the transaction context manager to run a transaction. This will automatically commit the transaction when  the context manager exits, or rollback it if an exception is raised.</p> <p><pre><code>async with pg.transaction() as tran:\n    await tran(\"DELETE FROM comments WHERE id = 1;\")\n    await tran(\n        \"INSERT INTO comments (content) VALUES (%s);\",\n        [(\"Comment 1\",), (\"Comment 2\",)],\n    )</code></pre> This will execute the two queries in a single transaction and then automatically commit it. If an error occurs, nothing  in the transaction scope will be applied, the connection returned to the pool and the error raised. Nothing from the  following block will be applied, for example: <pre><code>async with pg.transaction() as tran:\n    await tran(\"DELETE FROM comments WHERE id = 38;\")\n    await tran(\n        \"INSERT INTO comments (content) VALUES (%s);\",\n        [(\"Comment 1\",), (\"Comment 2\",)],\n    )\n    raise ValueError(\"The almighty Elephant has rejected your submission.\")</code></pre></p>"},{"location":"rationale/","title":"Rationale","text":"<p>When interacting with PostgreSQL databases in Python, developers typically choose between using an Object-Relational Mapper (ORM) like SQLAlchemy or Django ORM, or working directly with a database driver like <code>psycopg</code>.</p>"},{"location":"rationale/#object-relational-mappers-orms","title":"Object-Relational Mappers (ORMs)","text":"<p>ORMs offer significant advantages:</p> <ul> <li>Abstraction: They hide the complexities of SQL, allowing developers to work with Python objects and methods.</li> <li>Type Safety: Data interaction often benefits from static analysis and type checking.</li> <li>Editor Convenience: Features like autocompletion and refactoring are greatly enhanced.</li> <li>Reduced Boilerplate: Common database operations are simplified.</li> </ul> <p>However, ORMs also come with drawbacks:</p> <ul> <li>Learning Curve: Learning the ORMs specific syntax and behavior adds overhead and can result in having to resort   hacky workarounds in some places.</li> <li>Limited Flexibility: Complex or highly optimized SQL queries can be difficult or impossible to express through the   ORMs abstraction.</li> <li>Obscured SQL: It can be hard to predict or debug the exact SQL generated by the ORM.</li> <li>Performance: The abstraction layer can introduce performance overhead, especially for complex queries.</li> </ul>"},{"location":"rationale/#direct-sql-via-psycopg-provides","title":"Direct SQL (via <code>psycopg</code>) provides:","text":"<ul> <li>Full Control: Direct access to the full power and flexibility of SQL.</li> <li>Universality: SQL is a standard language, widely understood and applicable across different database systems.</li> <li>Performance: Ability to write highly optimized queries without abstraction layers.</li> <li>Flexibility: PostgreSQL-specific features and optimizations can be fully utilized.</li> </ul> <p>But this approach often involves:</p> <ul> <li>Verbosity: Writing raw SQL and handling connections, cursors, and data fetching can be verbose and repetitive.</li> <li>Lack of Type Safety: Mapping database results to Python types often requires manual effort and lacks build-time   checks.</li> <li>Limited Editor Support: Editor assistance for SQL strings within Python code is often minimal.</li> </ul>"},{"location":"rationale/#python-postgres","title":"Python Postgres","text":"<p>Among many excellent choices already out there, Python Postgres offers another take on balancing the trade-offs outlined above.</p> <p>It seeks to provide a developer experience that combines the type safety and editor convenience typically associated with ORMs, while retaining the power, flexibility, and universality of writing direct SQL.</p> <p>It achieves this by:</p> <ol> <li>It's just SQL: You write standard PostgreSQL queries, ensuring full control and leveraging existing SQL    knowledge.</li> <li>Pydantic: Results can be automatically parsed into Pydantic models, providing data validation, type    hints, and enhanced editor support (like autocompletion for result attributes). You can also pass Pydantic models    as query parameters.</li> <li>Reducing Boilerplate: Simplifying connection handling and common query patterns compared to using <code>psycopg</code>    directly.</li> </ol> <p>The goal is to offer a pragmatic middle ground: keep the SQL, but enhance the Python development experience around it, making database interactions more convenient, and less verbose without sacrificing the underlying power of SQL.</p>"},{"location":"api/client/","title":"Postgres Client","text":""},{"location":"api/client/#python_postgres.Postgres.__init__","title":"__init__","text":"<pre><code>__init__(\n    user: str,\n    password: str,\n    host: str,\n    port: int = 5432,\n    database: str = \"postgres\",\n    pool_min_size: int = 10,\n    pool_max_size: int = 50,\n    patch: Optional[AsyncConnectionPatch] = None,\n    name: str = \"python-postgres\",\n    timeout: float = 30.0,\n    max_waiting: int = 0,\n    max_lifetime: float = 60 * 60.0,\n    max_idle: float = 10 * 60.0,\n    reconnect_timeout: float = 5 * 60.0,\n)</code></pre> <p>Initialize the Postgres class to connect to a PostgreSQL database. This will create a connection Pool with the given parameters. The connection pool is not opened until the first query is executed. This has little performance impact, since you can use the first connection while the others are opened in the background and prevents prematurely acquiring connections that are not needed.</p> <p>Parameters:</p> Name Type Description Default <code>user</code> <code>str</code> <p>The username to connect to the database.</p> required <code>password</code> <code>str</code> <p>The password for the given user to connect to the database.</p> required <code>host</code> <code>str</code> <p>The host of the database.</p> required <code>port</code> <code>int</code> <p>The port of the database, default is 5432.</p> <code>5432</code> <code>database</code> <code>str</code> <p>The database name to connect to, default is <code>postgres</code>.</p> <code>'postgres'</code> <code>pool_min_size</code> <code>int</code> <p>The minimum number of connections to keep in the pool.</p> <code>10</code> <code>pool_max_size</code> <code>int</code> <p>The maximum number of connections to keep in the pool.</p> <code>50</code> <code>patch</code> <code>Optional[AsyncConnectionPatch]</code> <p>A list of functions to call on the connection after it is created. This is useful to set up the connection with custom settings, like enabling extensions.</p> <code>None</code> <code>name</code> <code>str</code> <p>An optional name to give to the connection pool, to identify it in the logs. Default to <code>python-postgres</code> to distinguish it from other pools that may be active.</p> <code>'python-postgres'</code> <code>timeout</code> <code>float</code> <p>The timeout in seconds to wait for a connection to be acquired from the pool. Default is 30 seconds.</p> <code>30.0</code> <code>max_waiting</code> <code>int</code> <p>The maximum number of waiting connections to allow. Default is 0, which means no limit.</p> <code>0</code> <code>max_lifetime</code> <code>float</code> <p>The maximum lifetime of a connection in seconds. Default is 60 minutes.</p> <code>60 * 60.0</code> <code>max_idle</code> <code>float</code> <p>The maximum idle time of a connection in seconds. Default is 10 minutes.</p> <code>10 * 60.0</code> <code>reconnect_timeout</code> <code>float</code> <p>The timeout in seconds to wait for a connection to be reconnected from the pool. Default is 5 minutes.</p> <code>5 * 60.0</code>"},{"location":"api/client/#python_postgres.Postgres.__call__","title":"__call__  <code>async</code>","text":"<pre><code>__call__(\n    query: Query,\n    params: Params = (),\n    binary: bool = True,\n    model: Optional[Type[T]] = None,\n    **kwargs,\n) -&gt; list[T] | list[tuple]</code></pre> <p>Execute a query and return the results, or the number of affected rows. You can pass any query to this method. The Connections in the pool are in <code>autocommit</code> mode by default. This means that changes to the database are automatically committed and generally is more performant. It further allows for Operations that cannot be called in a Transaction like <code>VACUUM</code> or <code>CALL</code>. If you want to execute queries in a Transaction context, use the <code>transaction</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Query</code> <p>The query to execute.</p> required <code>params</code> <code>Params</code> <p>The parameters to pass to the query.</p> <code>()</code> <code>binary</code> <code>bool</code> <p>Whether to use binary mode for the cursor. Default is True, which is more performant for most queries, but may not work with all queries. If you need to use text mode i.e. for type adapters, set this to False.</p> <code>True</code> <code>model</code> <code>Optional[Type[T]]</code> <p>The Pydantic model to parse the results into.</p> <code>None</code> <code>kwargs</code> <p>Keyword arguments passed to the Pydantic serialization method, such as <code>by_alias</code>, <code>exclude</code>, etc. This is usually the easiest way to make sure your model fits the table schema definition. <code>exclude_none</code> is always set.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[T] | list[tuple]</code> <p>The results of the query.</p>"},{"location":"api/client/#python_postgres.Postgres.one","title":"one  <code>async</code>","text":"<pre><code>one(\n    query: Query,\n    params: Params = (),\n    binary: bool = True,\n    model: Optional[Type[T]] = None,\n    **kwargs,\n) -&gt; T | tuple | None</code></pre> <p>Execute a query and return the first result, or None if no results are found. Otherwise, this behaves identically to the <code>__call__</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Query</code> <p>The query to execute.</p> required <code>params</code> <code>Params</code> <p>The parameters to pass to the query.</p> <code>()</code> <code>model</code> <code>Optional[Type[T]]</code> <p>The Pydantic model to parse the results into. If not provided, a new model with all columns in the query will be used.</p> <code>None</code> <code>binary</code> <code>bool</code> <p>Whether to use binary mode for the cursor. Default is True, which is more performant for most queries, but may not work with all queries. If you need to use text mode i.e. for type adapters, set this to False.</p> <code>True</code> <code>kwargs</code> <p>Keyword arguments passed to the Pydantic serialization method, such as <code>by_alias</code>, <code>exclude</code>, etc. This is usually the easiest way to make sure your model fits the table schema definition. <code>exclude_none</code> is always set.</p> <code>{}</code> <p>Returns:</p> Type Description <code>T | tuple | None</code> <p>The first result of the query, or None if there isn't one.</p>"},{"location":"api/client/#python_postgres.Postgres.insert","title":"insert  <code>async</code>","text":"<pre><code>insert(\n    table_name: LiteralString,\n    params: PydanticParams,\n    prepare: bool = False,\n    binary: bool = True,\n    returning: Optional[list[LiteralString]] = None,\n    **kwargs,\n) -&gt; list[tuple] | tuple | int</code></pre> <p>Dynamically expand an insert query to correctly handle pydantic models with optional fields, applying default values rather than explicitly passing <code>None</code> to the query. This always produces one single Query. The column names to insert are determined by all the non-<code>None</code> fields across all given models.</p> <p>This will not be particularly efficient for large inserts and solves a specific problem. If you have uniform models and can construct one query to achieve the same, you should prefer that.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>LiteralString</code> <p>The name of the table to insert into.</p> required <code>params</code> <code>PydanticParams</code> <p>The Pydantic model or list of models to insert.</p> required <code>prepare</code> <code>bool</code> <p>Whether to use prepared statements. Default is False, due to the dynamic nature and possibly rather large size of the query.</p> <code>False</code> <code>binary</code> <code>bool</code> <p>Whether to use binary mode for the cursor. Default is True, which is more performant for most queries, but may not work with all queries. If you need to use text mode i.e. for type adapters, set this to False.</p> <code>True</code> <code>returning</code> <code>Optional[list[LiteralString]]</code> <p>An optional list of Column Names to return after the insert.</p> <code>None</code> <code>kwargs</code> <p>Keyword arguments passed to the Pydantic serialization method, such as <code>by_alias</code>, <code>exclude</code>, etc. This is usually the easiest way to make sure your model fits the table schema definition. <code>exclude_none</code> is always set.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[tuple] | tuple | int</code> <p>If <code>returning</code> is provided, returns the specified columns. Otherwise, returns the number of rows affected by the insert.</p>"},{"location":"api/client/#python_postgres.Postgres.transaction","title":"transaction  <code>async</code>","text":"<pre><code>transaction(binary: bool = True) -&gt; AsyncIterator[Transaction]</code></pre> <p>Create a transaction context manager to execute multiple queries in a single transaction. You can call the transaction the same way you would call the <code>Postgres</code> instance itself.</p> <p>Parameters:</p> Name Type Description Default <code>binary</code> <code>bool</code> <p>Whether to use binary mode for the cursor. Default is True, which is more performant for most queries, but may not work with all queries. If you need to use text mode i.e. for type adapters, set this to False.</p> <code>True</code>"},{"location":"api/client/#python_postgres.Postgres.connection","title":"connection  <code>async</code>","text":"<pre><code>connection() -&gt; AsyncIterator[psycopg.AsyncConnection]</code></pre> <p>Acquire a psycopg AsyncConnection from the pool for direct use. The connection will be in autocommit mode by default.</p>"},{"location":"api/client/#python_postgres.Postgres.is_open","title":"is_open  <code>property</code>","text":"<pre><code>is_open: bool</code></pre> <p>Check if the pool is open and available for new clients.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the pool is open, False otherwise.</p>"},{"location":"api/client/#python_postgres.Postgres.close","title":"close  <code>async</code>","text":"<pre><code>close() -&gt; None</code></pre> <p>Close the pool and make it unavailable to new clients.</p> <p>All the waiting and future clients will fail to acquire a connection with a PoolClosed exception. Currently used connections will not be closed until returned to the pool.</p> <p>Wait timeout seconds for threads to terminate their job, if positive. If the timeout expires the pool is closed anyway, although it may raise some warnings on exit.</p>"},{"location":"guides/client/","title":"Applications","text":"<p>One of the main benefits of Python Postgres is the ability to run arbitrary SQL queries and having a simple pydantic model to represent the results. This is particularly handy when you do not want to maintain pydantic models for every table in your database, when you're just not interested in most of them but instead only in certain compositions of them.</p> Setup <pre><code>CREATE TABLE customers (\n  customer_id SERIAL PRIMARY KEY,\n  first_name VARCHAR(50) NOT NULL,\n  last_name VARCHAR(50) NOT NULL,\n  email VARCHAR(100) UNIQUE NOT NULL,\n  phone VARCHAR(20),\n  birth_date DATE,\n  address JSONB,\n  loyalty_points INTEGER DEFAULT 0,\n  segment VARCHAR(20) CHECK (segment IN ('STANDARD', 'PREMIUM', 'VIP')),\n  preferences TEXT[],\n  created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n  last_login TIMESTAMP WITH TIME ZONE,\n  is_active BOOLEAN DEFAULT TRUE\n);\n\nCREATE TABLE products (\n  product_id SERIAL PRIMARY KEY,\n  sku VARCHAR(20) UNIQUE NOT NULL,\n  name VARCHAR(100) NOT NULL,\n  description TEXT,\n  price DECIMAL(10, 2) NOT NULL CHECK (price &gt;= 0),\n  cost DECIMAL(10, 2) CHECK (cost &gt;= 0),\n  category VARCHAR(50) NOT NULL,\n  subcategory VARCHAR(50),\n  attributes JSONB,\n  stock_quantity INTEGER NOT NULL DEFAULT 0,\n  reorder_level INTEGER,\n  weight DECIMAL(8, 2),\n  dimensions VARCHAR(50),\n  is_discontinued BOOLEAN DEFAULT FALSE,\n  created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n);\n\nCREATE TABLE orders (\n  order_id SERIAL PRIMARY KEY,\n  customer_id INTEGER REFERENCES customers (customer_id),\n  order_date TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n  status VARCHAR(20) CHECK (\n    status IN (\n      'PENDING',\n      'PROCESSING',\n      'SHIPPED',\n      'DELIVERED',\n      'CANCELLED'\n    )\n  ),\n  total_amount DECIMAL(12, 2) NOT NULL,\n  discount_amount DECIMAL(12, 2) DEFAULT 0,\n  tax_amount DECIMAL(12, 2) DEFAULT 0,\n  shipping_address JSONB,\n  payment_method VARCHAR(50),\n  tracking_number VARCHAR(100),\n  notes TEXT,\n  items JSONB NOT NULL,\n  shipping_date TIMESTAMP WITH TIME ZONE,\n  delivery_date TIMESTAMP WITH TIME ZONE\n);</code></pre> <p>In real-world examples, you may often find queries like this:</p> Query <pre><code>SELECT\n    c.customer_id,\n    c.first_name || ' ' || c.last_name AS customer_name,\n    c.segment AS customer_segment,\n    o.order_date,\n    p.name AS top_product_name,\n    p.category,\n    SUM(o.total_amount) OVER (\n        PARTITION BY\n            c.customer_id\n        ) AS customer_lifetime_value,\n    RANK() OVER (\n        PARTITION BY\n            p.category\n        ORDER BY\n            o.total_amount DESC\n        ) AS category_rank\nFROM\n    customers c\n        INNER JOIN orders o ON c.customer_id = o.customer_id\n        INNER JOIN LATERAL (\n        SELECT\n            p.product_id,\n            p.name,\n            p.category,\n            p.price\n        FROM\n            products p\n        WHERE\n            p.product_id = ANY (\n                   SELECT\n                       (JSONB_ARRAY_ELEMENTS(o.items) -&gt;&gt; 'product_id')::INTEGER\n                   )\n        ORDER BY\n            p.price DESC\n        LIMIT\n            1\n        ) p ON TRUE\nWHERE\n    c.is_active = TRUE\n  AND o.status IN ('SHIPPED', 'DELIVERED')\n  AND o.order_date &gt;= CURRENT_DATE - INTERVAL '1 year'\nGROUP BY\n    c.customer_id,\n    c.first_name,\n    c.last_name,\n    c.segment,\n    o.order_date,\n    p.name,\n    p.category,\n    o.total_amount\nHAVING\n    SUM(o.total_amount) &gt; 1000\nORDER BY\n    customer_lifetime_value DESC,\n    order_date DESC\nLIMIT\n    100;</code></pre> <p>For some example values, this would yield:</p> customer_id customer_name customer_segment order_date top_product_name category customer_lifetime_value category_rank 2 Maria Garcia VIP 2025-04-25 09:31:34.621258 +00:00 Premium Laptop Electronics 1479.98 1 1 John Smith PREMIUM 2025-04-25 09:31:34.621258 +00:00 Premium Laptop Electronics 1349.99 2 <p>In this scenario, not only would it be quite cumbersome to maintain models for all the tables with appropriate types  and constraints, but rewriting this query in an ORM syntax would probably be somewhat tedious, if we're only  interested in the result of this query and not in the tables themselves.</p> Actually ... <p>You could of course use a view for the above, but that can both be unwanted for a whole number of reasons, and is besides the point of this illustrative example.</p> <p>With Python Postgres, you can simply run this query and get a pydantic model with the result:</p> <pre><code>class CustomerOrderAnalytics(BaseModel):\n    customer_id: int\n    customer_name: str\n    customer_segment: Literal[\"STANDARD\", \"PREMIUM\", \"VIP\"]\n    order_date: datetime\n    top_product_name: str\n    category: str\n    customer_lifetime_value: Decimal\n    category_rank: int\n\ndata = await pg(query, model=CustomerOrderAnalytics)\nprint(data)</code></pre> <pre><code>[\n    CustomerOrderAnalytics(\n        customer_id=2,\n        customer_name=\"Maria Garcia\",\n        customer_segment=\"VIP\",\n        order_date=datetime.datetime(\n            2025, 4, 25, 9, 31, 34, 621258, tzinfo=datetime.timezone.utc\n        ),\n        top_product_name=\"Premium Laptop\",\n        category=\"Electronics\",\n        customer_lifetime_value=Decimal(\"1479.98\"),\n        category_rank=1,\n    ),\n    CustomerOrderAnalytics(\n        customer_id=1,\n        customer_name=\"John Smith\",\n        customer_segment=\"PREMIUM\",\n        order_date=datetime.datetime(\n            2025, 4, 25, 9, 31, 34, 621258, tzinfo=datetime.timezone.utc\n        ),\n        top_product_name=\"Premium Laptop\",\n        category=\"Electronics\",\n        customer_lifetime_value=Decimal(\"1349.99\"),\n        category_rank=2,\n    ),\n]</code></pre>"},{"location":"guides/extensions/","title":"Extensions","text":"<p>You can use extensions that require extending the driver by for e.g. adding additional types. If the extension you're interested in provides such an extension compatible with psycopg <code>AsyncConnection</code>, you can use it as you would with psycopg directly. You can pass these extensions to the <code>patch</code> parameter when instantiating your instance of <code>Postgres</code>.</p> <pre><code>pg = Postgres(\n    \"postgres\", \"password\", \"localhost\", patch=[register_my_extension]\n)</code></pre>"},{"location":"guides/extensions/#example-pgvector","title":"Example: pgvector","text":"<p><pre><code>import numpy as np\nfrom pydantic import BaseModel, ConfigDict\nfrom pgvector.psycopg import register_vector_async\n\n\nclass Item(BaseModel):\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    id: int\n    embedding: np.ndarray\n\npg = Postgres(\n    \"postgres\", \"password\", \"localhost\", patch=[register_vector_async]\n)\n\nasync def main():\n    await pg(\n        \"CREATE TABLE items (id bigserial PRIMARY KEY, embedding vector(3));\"\n    )\n    embedding = np.array([1, 2, 3])\n    await pg(\"INSERT INTO items (embedding) VALUES (%s)\", (embedding,))\n    res = await pg(\n        \"SELECT * FROM items ORDER BY embedding &lt;=&gt; %s;\",\n        (embedding,),\n        model=Item,\n    )\n    print(res)</code></pre> <pre><code>[Item(id=1, embedding=array([1., 2., 3.], dtype=float32))]</code></pre></p>"}]}